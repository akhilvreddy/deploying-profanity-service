{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f4494d0",
   "metadata": {},
   "source": [
    "# Serving a simple ML model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667c5422",
   "metadata": {},
   "source": [
    "I wanted to start by training a simple model, a profanity detection model. I wanted to use TF-IDF as explained in my blog post. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccfa486",
   "metadata": {},
   "source": [
    "In this part, we're going to be in `/model`. Let's start by generating our data using the raw data that's available on github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a43d5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23bb67c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['python', 'generate_data.py'], returncode=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "subprocess.run([\"python\", \"generate_data.py\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1927b719",
   "metadata": {},
   "source": [
    "And that generated a cleaned dataset for us to use at `data/tweets.csv`. We can use that for training now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e0ef1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model + val data saved.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['python', 'train.py'], returncode=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.run([\"python\", \"train.py\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5637ea18",
   "metadata": {},
   "source": [
    "In that file, we used a pretty simple TF-IDF model (as explained in the blog post). I set it up such that the model is saved as a `.joblib` file in the /app directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8beb4a89",
   "metadata": {},
   "source": [
    "Let's run some quick evals so we can make sure that the model isn't horrible (I'm not *too* concerned about eval metrics but I also don't want a garbage model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af0e7a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_val shape: (4954,)\n",
      "y_val shape: (4954,)\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.66      0.76       870\n",
      "           1       0.93      0.98      0.96      4084\n",
      "\n",
      "    accuracy                           0.93      4954\n",
      "   macro avg       0.91      0.82      0.86      4954\n",
      "weighted avg       0.92      0.93      0.92      4954\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 578  292]\n",
      " [  74 4010]]\n",
      "Validation Accuracy: 0.9261203068227695\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['python', 'evaluate.py'], returncode=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.run([\"python\", \"evaluate.py\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4695a2",
   "metadata": {},
   "source": [
    "Okay so this means that our model isn't horrible (acutally it's not bad at all - ~93% accuracy and decent F1). Let's try it on a couple of examples so that we know it's actually fine (and will work for people who want to hit this API eventually)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4255cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputSentence1 = \"this is so fucking boring\"\n",
    "inputSentence2 = \"what a bitch bruh\"\n",
    "inputSentence3 = \"this ice cream is actually amazing\"\n",
    "\n",
    "inputs = [inputSentence1, inputSentence2, inputSentence3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c2a31a",
   "metadata": {},
   "source": [
    "But we have to switch directories to `/app` first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b4f5d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../app\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "793cd333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stdout 1: Text: this is so fucking boring\n",
      "is_profane: True | confidence: 91.59%\n",
      "stdout 2: Text: what a bitch bruh\n",
      "is_profane: True | confidence: 98.0%\n",
      "stdout 3: Text: this ice cream is actually amazing\n",
      "is_profane: False | confidence: 38.22%\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "for i, sentence in enumerate(inputs, start=1):\n",
    "    result = subprocess.run(\n",
    "        [\"python\", \"check_model.py\", sentence],\n",
    "        capture_output=True, text=True\n",
    "    )\n",
    "    print(f\"stdout {i}:\", result.stdout.strip())\n",
    "    if result.stderr:\n",
    "        print(f\"stderr {i}:\", result.stderr.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3330c7c1",
   "metadata": {},
   "source": [
    "That was a nice sanity check and we can see that any inappropriate word is instantly flagged with high confidence. \n",
    "\n",
    "\n",
    "Our machine learning is done now and for the rest we are going to focus on engineering a system around serving this model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d22dbfd",
   "metadata": {},
   "source": [
    "## Bentoml Deployment\n",
    "\n",
    "In the blog post, I talked about how much bentoml helps. I'll now show how much it helps in action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea33343e",
   "metadata": {},
   "source": [
    "If you take a look at `app/service_bento.py` we can see that we loaded our model, set up a service, and made an endpoint of requests to hit. We can start testing it by servign it locally. \n",
    "\n",
    "Ideally you would do\n",
    "\n",
    "```bash\n",
    "cd app\n",
    "bentoml serve service_bento:svc\n",
    "```\n",
    "\n",
    "but since we are in a .ipynb I'm going to try to make subprocess run that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4888efbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(tag=\"profanity_detector:att6b7ddiwqbxbxv\", path=\"/Users/akhilvreddy/bentoml/models/profanity_detector/att6b7ddiwqbxbxv/\")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "import bentoml\n",
    "\n",
    "pipeline = joblib.load(\"profanity.joblib\")\n",
    "bentoml.sklearn.save_model(\"profanity_detector\", pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c924a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akhilvreddy/Documents/deploying-profanity-service/.venv/lib/python3.13/site-packages/fs/__init__.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  __import__(\"pkg_resources\").declare_namespace(__name__)  # type: ignore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-17T15:34:16-0400 [INFO] [cli] Starting production HTTP BentoServer from \"service_bento:ProfanityService\" listening on http://localhost:3000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akhilvreddy/Documents/deploying-profanity-service/.venv/lib/python3.13/site-packages/fs/__init__.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  __import__(\"pkg_resources\").declare_namespace(__name__)  # type: ignore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-17T15:34:17-0400 [INFO] [entry_service:ProfanityService:1] Service ProfanityService initialized\n"
     ]
    }
   ],
   "source": [
    "process = subprocess.Popen(\n",
    "    [\"bentoml\", \"serve\", \"service_bento:ProfanityService\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76748de9",
   "metadata": {},
   "source": [
    "Let's make some API calls now to make sure it's working well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3201f054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-17T15:37:05-0400 [INFO] [entry_service:ProfanityService:1] 127.0.0.1:55281 (scheme=http,method=POST,path=/predict,type=application/json,length=31) (status=200,type=application/json,length=42) 1.779ms (trace=f319d0ad54bea97cbb1cb3b7ef535f2d,span=be13ee9505ec893e,sampled=0,service.name=ProfanityService)\n",
      "Status Code: 200\n",
      "Response: {'is_profane': True, 'confidence': 0.9841}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://localhost:3000/predict\"\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "data = {\"text\": \"you fucking dumbass\"}\n",
    "\n",
    "response = requests.post(url, json=data, headers=headers)\n",
    "\n",
    "print(\"Status Code:\", response.status_code)\n",
    "print(\"Response:\", response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08184187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-17T15:37:58-0400 [INFO] [entry_service:ProfanityService:1] 127.0.0.1:55300 (scheme=http,method=POST,path=/predict,type=application/json,length=29) (status=200,type=application/json,length=43) 1.587ms (trace=02ed65ae8554d29b5eb16f3fbb93da31,span=15043a3ec72bc5ef,sampled=0,service.name=ProfanityService)\n",
      "Status Code: 200\n",
      "Response: {'is_profane': False, 'confidence': 0.3354}\n"
     ]
    }
   ],
   "source": [
    "data = {\"text\": \"ice cream is good\"}\n",
    "\n",
    "response = requests.post(url, json=data, headers=headers)\n",
    "\n",
    "print(\"Status Code:\", response.status_code)\n",
    "print(\"Response:\", response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f9c8c8",
   "metadata": {},
   "source": [
    "Okay so that's good too. We're hitting our local API and getting responses just like we would've had we just used the model in eval mode.\n",
    "\n",
    "The next part would be to actually ship this as a containerized API with the model on a registry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21baee40",
   "metadata": {},
   "source": [
    "We would have to do a `bento build` first which would create a `bento/` directory that contains model and service. Then, we can containerize. \n",
    "\n",
    "Again, ideally I would've like to do\n",
    "\n",
    "```bash\n",
    "bentoml build\n",
    "bentoml containerize profanity_api:latest\n",
    "```\n",
    "\n",
    "but we have to use our workaround."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d043256a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akhilvreddy/Documents/deploying-profanity-service/.venv/lib/python3.13/site-packages/fs/__init__.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  __import__(\"pkg_resources\").declare_namespace(__name__)  # type: ignore\n",
      "Error: [bentos] `build` failed: Failed to load bento or import service ''. The directory '/Users/akhilvreddy/Documents/deploying-profanity-service/app' does not contain a valid bentofile.yaml or service.py.\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['bentoml', 'build']' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCalledProcessError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msubprocess\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbentoml\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbuild\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m subprocess.run([\u001b[33m\"\u001b[39m\u001b[33mbentoml\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontainerize\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mprofanity_api:latest\u001b[39m\u001b[33m\"\u001b[39m], check=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/subprocess.py:579\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[39m\n\u001b[32m    577\u001b[39m     retcode = process.poll()\n\u001b[32m    578\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m check \u001b[38;5;129;01mand\u001b[39;00m retcode:\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(retcode, process.args,\n\u001b[32m    580\u001b[39m                                  output=stdout, stderr=stderr)\n\u001b[32m    581\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m CompletedProcess(process.args, retcode, stdout, stderr)\n",
      "\u001b[31mCalledProcessError\u001b[39m: Command '['bentoml', 'build']' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "subprocess.run([\"bentoml\", \"build\"], check=True)\n",
    "subprocess.run([\"bentoml\", \"containerize\", \"profanity_api:latest\"], check=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb05894",
   "metadata": {},
   "source": [
    "Ignore the above error - I ran the rest on my local terminal and I'll attach pictures.\n",
    "\n",
    "I'll just talk through the next few steps because it makes 0 sense to run these from ipynb python cells. \n",
    "\n",
    "After those two commands, I created a docker container with our model that's being served by calling\n",
    "\n",
    "```bash\n",
    "bentoml containerize profanity_service:p2cgpvddjc4szbxv \n",
    "```\n",
    "\n",
    "Optionally, I could have pushed this to a model registry (if I was deploying this as part of a true service on production) with:\n",
    "\n",
    "```bash\n",
    "bentoml push profanity_service:p2cgpvddjc4szbxv \n",
    "```\n",
    "\n",
    "this would've pushed my local image to bento's cloud and this would've worked as a registry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcfc9b6",
   "metadata": {},
   "source": [
    "Here's some results: \n",
    "\n",
    "![one](../assets/pic1.png)\n",
    "![two](../assets/pic2.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a557e3",
   "metadata": {},
   "source": [
    "To recap, we went from:\n",
    "\n",
    "1) Raw data\n",
    "2) Model\n",
    "3) Service\n",
    "4) Bento\n",
    "5) Docker container\n",
    "\n",
    "Referring back to what I had on my blog, we can wrap this up by pushing to GHCR, Docker Hub, or ECR. We're basically in the \"lock and ship\" phase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f15e6e5",
   "metadata": {},
   "source": [
    "I'm going to do GHCR for simplicity (well integrated with the ecosystem already). Here's the packages getting uploaded:\n",
    "\n",
    "![yo](../assets/pic3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbbd7f3",
   "metadata": {},
   "source": [
    "So once this has been uploaded, it means that any machine that can authenticate to GHCR can now run `docker pull` and run this API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0badb3f",
   "metadata": {},
   "source": [
    "Let's move to the last part, which is deploying this on the cloud. I'm going to go with fly.io sicne it's free to start."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8f13c3",
   "metadata": {},
   "source": [
    "Essentially, I want fly.io to host and serve my dockerized app without me having to worry about the infra overhead. It's supposed to run it on a public server and usually returns a URL for you to make calls to your service. \n",
    "\n",
    "For this case, we should expect a link like https://profanity-service.fly.dev/predict and now anyone can now curl or `requests.post()` to that endpoint and hit that API. **I would say the work here is done when we can curl a phrase with profanity to that link straight from my terminal.** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9deb034",
   "metadata": {},
   "source": [
    "After this, I ran some commands to set up fly:\n",
    "\n",
    "```bash\n",
    "flyctl deploy --image ghcr.io/akhilvreddy/profanity_service:latest\n",
    "```\n",
    "\n",
    "That deployed my base image to fly. However, I quickally got an email saying that I ran out of compute because of how big the application is. And that makes sense because this isn't a tiny microserivce - it's a logistic regression model that's trained on a decent amount of data. \n",
    "\n",
    "I had to scale it up to serve\n",
    "\n",
    "```bash\n",
    "fly scale memory 512 -a deploying-profanity-service\n",
    "```\n",
    "\n",
    "and then to start the application again\n",
    "\n",
    "```bash\n",
    "flyctl start -a deploying-profanity-service\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c6265c",
   "metadata": {},
   "source": [
    "And that worked for me! Here's some proof\n",
    "\n",
    "\n",
    "The page (https://profanity-service.fly.dev/predict) opened to a swagger page:\n",
    "\n",
    "![](../assets/pic4.png)\n",
    "\n",
    "And as we can see, it is predicting quickly! \n",
    "\n",
    "Here's what the whole page looked like:\n",
    "\n",
    "![](../assets/pic5.png)\n",
    "\n",
    "As I mentioned in the blog post, BentoML automatically gives us those health endpoints which is so useful.\n",
    "\n",
    "Here's proof that it was working on my phone as well:\n",
    "\n",
    "![](../assets/pic6.png)\n",
    "\n",
    "And it was super nice to see that logs in prometheus were automatically generated:\n",
    "\n",
    "![](../assets/pic7.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be42fbcc",
   "metadata": {},
   "source": [
    "Okay so the app is fully hosted, and the last (but most important part) would be to set up a re-training loop. I currently don't have any way to get the \"latest profane words\" so I'll simulate a pipeline given the fact that I have some new data coming in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68c66bb",
   "metadata": {},
   "source": [
    "Here's a pipeline that we would use in practice to fully automate this loop\n",
    "\n",
    "deploy.yaml\n",
    "```yaml\n",
    "name: Retrain, Containerize & Deploy\n",
    "\n",
    "on:\n",
    "  push:\n",
    "    branches: [main]\n",
    "    paths:\n",
    "      # retrigger pipeline if relevant files changed\n",
    "      - \"app/train.py\"\n",
    "      - \"app/eval.py\"\n",
    "      - \"app/generate_data.py\"\n",
    "      - \"data/**\"\n",
    "      - \"app/service_bento.py\"\n",
    "      - \"app/bentofile.yaml\"\n",
    "\n",
    "  # nightly drift detection\n",
    "  schedule:\n",
    "    - cron: '0 4 * * *'\n",
    "\n",
    "  # manual trigger support\n",
    "  workflow_dispatch: {}\n",
    "\n",
    "jobs:\n",
    "  deploy:\n",
    "    runs-on: ubuntu-latest\n",
    "\n",
    "    permissions:\n",
    "      contents: read\n",
    "      packages: write\n",
    "\n",
    "    steps:\n",
    "      # checkout and setup\n",
    "      - name: Checkout repo\n",
    "        uses: actions/checkout@v4\n",
    "\n",
    "      - name: Set up Python\n",
    "        uses: actions/setup-python@v5\n",
    "        with:\n",
    "          python-version: \"3.11\"\n",
    "          cache: pip\n",
    "\n",
    "      - name: Install dependencies\n",
    "        run: |\n",
    "          pip install --upgrade pip\n",
    "          pip install -r requirements.txt\n",
    "\n",
    "      # once new data is in, generate a new cleaned file\n",
    "      - name: Generate training data\n",
    "        run: python app/generate_data.py\n",
    "\n",
    "      # retrain & evaluate Model\n",
    "      - name: Train model\n",
    "        run: python app/train.py\n",
    "\n",
    "      - name: Evaluate model quality\n",
    "        run: python app/evaluate.py\n",
    "\n",
    "      # in production, fail here if performance drops too low\n",
    "      # like if evaluate.py results in bad F1 or accuracy\n",
    "\n",
    "      # BentoML\n",
    "      - name: Set up BentoML\n",
    "        uses: bentoml/setup-bentoml-action@v1\n",
    "        with:\n",
    "          python-version: \"3.11\"\n",
    "\n",
    "      - name: Build Bento\n",
    "        run: bentoml build\n",
    "\n",
    "      - name: Login to GitHub Container Registry\n",
    "        uses: docker/login-action@v3\n",
    "        with:\n",
    "          registry: ghcr.io\n",
    "          username: ${{ github.repository_owner }}\n",
    "          password: ${{ secrets.GITHUB_TOKEN }}\n",
    "\n",
    "      - name: Containerize & push to GHCR\n",
    "        uses: bentoml/containerize-push-action@v1\n",
    "        with:\n",
    "          bento-tag: profanity_service:latest\n",
    "          push: true\n",
    "          tags: ghcr.io/${{ github.repository_owner }}/profanity_service:latest\n",
    "          platform: linux/amd64\n",
    "\n",
    "      # deploy to Fly.io      \n",
    "      - name: Install flyctl\n",
    "        uses: superfly/flyctl-actions/setup-flyctl@master\n",
    "\n",
    "      - name: Deploy to Fly.io\n",
    "        run: flyctl deploy --image ghcr.io/${{ github.repository_owner }}/profanity_service:latest --remote-only\n",
    "        env:\n",
    "          FLY_API_TOKEN: ${{ secrets.FLY_API_TOKEN }}\n",
    "\n",
    "      # drift monitoring and prometheus hook\n",
    "      - name: (Optional) Send Prometheus Drift Metric\n",
    "        if: github.event_name == 'schedule'\n",
    "        run: |\n",
    "          echo \"::notice ::Insert Prometheus drift check & pushgateway curl here\"\n",
    "          # e.g. curl -X POST http://prometheus-server/push/...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bbb463",
   "metadata": {},
   "source": [
    "That gives us an end-to-end pipeline that would retrain, evaluate, containerize, and serve our application again. This is a bare-bones version but in production settings companies probably have thousands of pipelines like this running on various triggers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
